[
["index.html", "Introducción al aprendizaje no supervisado Introducción", " Introducción al aprendizaje no supervisado Juan C. Laria 2020-03-04 Introducción Utilizaremos los datos de Weber and Weber (1974) sobre consumo de proteínas en los países europeos en la década de los ’70 del pasado siglo. Comenzamos por cargar los datos en R. data.protein = read.csv(&quot;data/protein.csv&quot;) En este caso, el archivo que queremos importar en R se encuentra en un recurso web, en lugar de la ruta local usual. Una vez importados los datos, podemos ver de qué se tratan. library(dplyr) library(magrittr) data.protein %&gt;% head ## Country RedMeat WhiteMeat Eggs Milk Fish Cereals Starch Nuts Fr.Veg ## 1 Albania 10.1 1.4 0.5 8.9 0.2 42.3 0.6 5.5 1.7 ## 2 Austria 8.9 14.0 4.3 19.9 2.1 28.0 3.6 1.3 4.3 ## 3 Belgium 13.5 9.3 4.1 17.5 4.5 26.6 5.7 2.1 4.0 ## 4 Bulgaria 7.8 6.0 1.6 8.3 1.2 56.7 1.1 3.7 4.2 ## 5 Czechoslovakia 9.7 11.4 2.8 12.5 2.0 34.3 5.0 1.1 4.0 ## 6 Denmark 10.6 10.8 3.7 25.0 9.9 21.9 4.8 0.7 2.4 dim(data.protein) ## [1] 25 10 Tenemos 25 países y 10 variables, aunque una de estas variables es Country, que contiene el identificador de cada fila. Debemos transformar este dataset para deshacernos de la columna Country sin perder la información de las filas. Esto lo hacemos con el comando rownames (que es de lectura y asignación). rownames(data.protein) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; ## [16] &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; rownames(data.protein) = data.protein$Country Si ahora vemos nuestro dataset, observamos que la información de la variable Country está repetida en los nombres de las filas. data.protein %&gt;% head ## Country RedMeat WhiteMeat Eggs Milk Fish Cereals Starch ## Albania Albania 10.1 1.4 0.5 8.9 0.2 42.3 0.6 ## Austria Austria 8.9 14.0 4.3 19.9 2.1 28.0 3.6 ## Belgium Belgium 13.5 9.3 4.1 17.5 4.5 26.6 5.7 ## Bulgaria Bulgaria 7.8 6.0 1.6 8.3 1.2 56.7 1.1 ## Czechoslovakia Czechoslovakia 9.7 11.4 2.8 12.5 2.0 34.3 5.0 ## Denmark Denmark 10.6 10.8 3.7 25.0 9.9 21.9 4.8 ## Nuts Fr.Veg ## Albania 5.5 1.7 ## Austria 1.3 4.3 ## Belgium 2.1 4.0 ## Bulgaria 3.7 4.2 ## Czechoslovakia 1.1 4.0 ## Denmark 0.7 2.4 Por tanto, podemos eliminar esa columna data.protein$Country = NULL Como podemos observar, aunque posiblemente las variables estén medidas en las mismas unidades, no se encuentran en el mismo rango de valores. Por ejemplo, Cereals toma valores altos comparada con el resto. Para evitar que la escala individual de las variables afecten el resultado final, escalamos previamente la matriz de datos. data.protein %&lt;&gt;% scale %&gt;% as.data.frame data.protein %&gt;% head ## RedMeat WhiteMeat Eggs Milk Fish ## Albania 0.08126490 -1.7584889 -2.1796385 -1.15573814 -1.20028213 ## Austria -0.27725673 1.6523731 1.2204544 0.39237676 -0.64187467 ## Belgium 1.09707621 0.3800675 1.0415022 0.05460623 0.06348211 ## Bulgaria -0.60590157 -0.5132535 -1.1954011 -1.24018077 -0.90638347 ## Czechoslovakia -0.03824231 0.9485445 -0.1216875 -0.64908235 -0.67126454 ## Denmark 0.23064892 0.7861225 0.6835976 1.11013912 1.65053488 ## Cereals Starch Nuts Fr.Veg ## Albania 0.9159176 -2.2495772 1.2227536 -1.35040507 ## Austria -0.3870690 -0.4136872 -0.8923886 0.09091397 ## Belgium -0.5146342 0.8714358 -0.4895043 -0.07539207 ## Bulgaria 2.2280161 -1.9435955 0.3162641 0.03547862 ## Czechoslovakia 0.1869740 0.4430614 -0.9931096 -0.07539207 ## Denmark -0.9428885 0.3206688 -1.1945517 -0.96235764 References "],
["kmeans.html", "Capítulo 1 Clústers partitivos 1.1 K medias 1.2 Librería factoextra 1.3 Variación de información 1.4 Minibatch kmeans 1.5 K mediano 1.6 Selección de K", " Capítulo 1 Clústers partitivos 1.1 K medias K-means es, probablemente, uno de los algoritmos de clúster más eficientes computacionalmente. Sin embargo, tiene dos desventajas fundamentales. Se considera únicamente la distancia euclídea. Hay que especificar desde el comienzo el número de clusters que queremos. El funcionamiento de este algoritmo se ilustra aquí. Para hacer k-medias en R utilizamos la función kmeans del paquete base stats. En este caso, especificaremos 5 clusters. km = kmeans(data.protein, centers = 5) km$cluster ## Albania Austria Belgium Bulgaria Czechoslovakia ## 1 2 2 1 3 ## Denmark E Germany Finland France Greece ## 5 3 4 2 1 ## Hungary Ireland Italy Netherlands Norway ## 3 2 1 2 5 ## Poland Portugal Romania Spain Sweden ## 3 1 1 1 5 ## Switzerland UK USSR W Germany Yugoslavia ## 2 2 3 2 1 Como vemos, la función kmeans devuelve una agrupación, de hecho, km$cluster tiene una estructura que comparten todos los algoritmos de agrupación (observación, clúster). 1.2 Librería factoextra factoextra es un paquete para extraer y visualizar las salidas de distintos análisis exploratorios en R. Para visualizar los grupos resultantes, podemos utilizar la función fviz_cluster de la librería factoextra. Esta función visualizará los datos utilizando las dos primeras componentes principales. library(factoextra) fviz_cluster(km, data=data.protein, labelsize = 8) # Interactivo library(plotly) fviz_cluster(km, data=data.protein, labelsize = 8)%&gt;% ggplotly() 1.3 Variación de información ¿Cómo podríamos medir cuán diferentes son dos agrupaciones dadas sobre los mismos datos? Por ejemplo, la salida de km$cluster escalando y sin escalar los datos. La variación de información proporciona una medida para decir cuánta información comparten (o en este caso, no comparten) dos particiones de los datos. De hecho, es una distancia en el sentido matemático estricto de distancia. cluster_scale &lt;- km$cluster # Repetimos el mismo análisis pero sin escalar data.protein = read.csv(&quot;data/protein.csv&quot;) rownames(data.protein) = data.protein$Country data.protein$Country = NULL km = kmeans(data.protein, centers = 5) cluster_no_scale &lt;- km$cluster El paquete mcclust incluye una función para calcular la variación de información. library(mcclust) vi.dist(cluster_scale, cluster_no_scale, parts = TRUE) ## vi H(1|2) H(2|1) ## 1.5843856 0.8105865 0.7737991 ¿ Con una agrupación aleatoria qué obtendríamos ? cluster_rand &lt;- sample(5, length(cluster_scale), replace = TRUE) vi.dist(cluster_rand, cluster_scale) ## [1] 3.419358 1.4 Minibatch kmeans MiniBatch k-means ha sido propuesto como una alternativa al algoritmo k-means para agrupar datos masivos. La ventaja de MiniBatch k-means es que reduce el coste computacional al no utilizar todos los datos en cada iteración, sino una muestra aleatoria de tamaño fijo. En esta sección ilustraremos el uso de MiniBatch k-means en segmentación de imágenes (Color Quantization). Utilizaremos las librerías de R ClusterR para el algoritmo, y OpenImageR para representar las imágenes. library(ClusterR) library(OpenImageR) En este ejemplo utilizaremos una imagen de arte rupestre. Primero descargamos y leemos la imagen. img = readImage(&quot;data/beach.png&quot;) dim(img) ## [1] 299 299 3 Podemos observar que la imagen tiene una resolución de 299, 299, 3 píxeles, separados en tres canales de colores (RGB), por lo que en realidad tenemos 8.940110^{4} datos y 3 variables. Para mostrar la imagen en pantalla utilizamos la función imageShow. imageShow(img) # img &lt;- RGB_to_HSV(img) A continuación, convertimos nuestra matriz 381x514x3 en otra matriz de dimensión 195834x3. img.vector = apply(img, 3, as.vector) dim(img.vector) ## [1] 89401 3 Ya estamos en condiciones de hacer MiniBatch K-means a nuestra imagen. mbkm = MiniBatchKmeans(img.vector, clusters = 3) A continuación, vamos a sustituir la información de cada píxel por el centro del clúster al que pertenece. De esta forma obtendremos una imagen que tiene solamente 10 colores. mb.pred = predict_MBatchKMeans(img.vector, mbkm$centroids) new.img = mbkm$centroids[mb.pred, ] Devolvemos la imagen a su estructura original para poderla representar. dim(new.img) = c(nrow(img), ncol(img), 3) imageShow(new.img) 1.5 K mediano Partition around medoids puede verse como una versión robusta de kmeans, ya que los centroides son observaciones en lugar de promedios de observaciones. Esto da lugar a clusters más interpretables. Adicionalmente, su implementación tiene ventajas con respecto a kmeans, por ejemplo, no se limita a la distancia euclídea. Para este algoritmo, utilizaremos la función pam de la librería cluster. library(cluster) pm = pam(data.protein, k=5) knitr::kable(pm$medoids) RedMeat WhiteMeat Eggs Milk Fish Cereals Starch Nuts Fr.Veg Italy 9.0 5.1 2.9 13.7 3.4 36.8 2.1 4.3 6.7 Switzerland 13.1 10.1 3.1 23.8 2.3 25.6 2.8 2.4 4.9 Yugoslavia 4.4 5.0 1.2 9.5 0.6 55.9 3.0 5.7 3.2 Sweden 9.9 7.8 3.5 24.7 7.5 19.5 3.7 1.4 2.0 Spain 7.1 3.4 3.1 8.6 7.0 29.2 5.7 5.9 7.2 El valor que devuelve la función pam es similar al que devuelve kmeans, excepto que en lugar de tener la propiedad centers, ahora tenemos medoids. Podemos visualizar la agrupación resultante. fviz_cluster(pm, data=data.protein, labelsize = 8) 1.5.1 CusterR::clara Como es lógico, pam es computacionalmente más costoso que k-means, pues calcular el medoid es mucho más difícil que hacer un promedio. Existe una alternativa eficiente a k-medoids, clara para agrupar grandes volúmenes de datos. Se basa en agrupar primero una muestra de los datos originales y luego asignar los datos restantes a estos grupos. Utilizaremos la función Clara_Medoids del paquete ClusterR, porque vamos a trabajar con la misma foto que usamos para ilustrar MiniBatchKmeans. clara.m = Clara_Medoids(img.vector, clusters = 10, samples = 10, sample_size = 0.001) Hay que ser muy cuidadosos aquí con los parámetros, sample y sample_size, porque tenemos muchos datos, y esta función es muy costosa computacionalmente si aumentamos estos parámetros. A continuación asignamos cada pixel a su color correspondiente, y mostramos la nueva imagen. clara.pred = predict_Medoids(img.vector, clara.m$medoids) new.img = clara.m$medoids[clara.pred$clusters, ] dim(new.img) = c(nrow(img), ncol(img), 3) imageShow(new.img) 1.6 Selección de K En esta sección estudiaremos tres técnicas para calcular el número óptimo de clusters. Primero, vamos a sacar una muestra de nuestros datos de menos tamaño, ya que los cálculos que haremos serán más costosos. set.seed(1) img.vector.small = img.vector[sample(nrow(img.vector),1000),] 1.6.1 Método del codo La idea básica de los algoritmos partitivos es obtener cluster con la mínima WSS (within-cluster sum of squares), que mide cuán compactos son los clusters. Pudiéramos intentar quedarnos con el número de clusters nclust que minimiza este valor. Sin embargo, WSS siempre decrece a medida que consideramos un mayor número de grupos. El método del codo mira el valor de WSS con respecto al número de grupos considerados, y busca el primer punto en que hay un cambio brusco en la curva, es decir, que adicionar un grupo nuevo no mejora demasiado con respecto a lo que ya había. library(factoextra) fviz_nbclust(img.vector.small, kmeans, method = &quot;wss&quot;) 1.6.2 Método average silhouette La idea básica de este método es medir la calidad de la agrupación en función de cuán bien encierra los datos en los diferentes grupos. ¿Debería cambiar mucho la silueta del cluster si quitamos alguna de sus observaciones? El método de la silueta promedio calcula la silueta de los grupos para distintos números de grupos nclust. El mejor número es aquel que maximiza la silueta. fviz_nbclust(img.vector.small, kmeans, method = &quot;silhouette&quot;) 1.6.3 Método Gap statistic Este se puede considerar el más formal de los métodos, y puede ser aplicado a cualquier método de clustering, incluyendo clúster jerárquico. set.seed(3) gap_stat = cluster::clusGap(img.vector.small, FUN = kmeans, K.max = 20) fviz_gap_stat(gap_stat, maxSE = list(method=&quot;globalmax&quot;)) "],
["clústers-jerárquicos.html", "Capítulo 2 Clústers jerárquicos 2.1 hclust 2.2 Librería protoclust", " Capítulo 2 Clústers jerárquicos 2.1 hclust Para crear un cluster jerarquico aglomerativo utilizaremos la función hclust del paquete básico stats. La sintaxis de esta función es hclust(d, method), donde d es una matriz de distancias entre las observaciones y method ( función linkage) describe el criterio que usaremos para unir distintos clusters. Para calcular la matriz de distancias d podemos usar la función dist, en este caso, dist(data.protein, method = \"euclidean\"), donde el parámetro method describe qué distancia estamos calculando. Esta no es la única función que podemos utilizar para calcular distancias. Por ejemplo, si nuestros datos son geográficos (longitud, latitud), usaríamos la función distm del paquete geosphere. hc = hclust(dist(data.protein, method = &quot;euclidean&quot;), method = &quot;complete&quot;); hc ## ## Call: ## hclust(d = dist(data.protein, method = &quot;euclidean&quot;), method = &quot;complete&quot;) ## ## Cluster method : complete ## Distance : euclidean ## Number of objects: 25 Podemos utilizar el método plot de nuestro cluster hc, con el parámetro hang=-1, que dibuja todas las etiquetas al mismo nivel. plot(hc, hang = -1, cex=0.8) Lo que R ha dibujado ha sido el dendograma de hc. Este gráfico se utiliza para describir la asignación de los clusters para cada valor de Height. Cuanto más cerca del cero se juntan las observaciones, en este caso los países, más similares son en cuanto a consumo de proteínas. Para cada valor específico de Height tenemos una asignación diferente de los clusters. 2.1.1 Distancias. ¿Qué hace la función dist? Vamos a explorar el resultado de la función dist aplicada a nuestros datos. d = dist(data.protein, method = &quot;euclidean&quot;); head(d) ## [1] 6.136051 5.948761 2.764537 5.141148 6.634162 6.392250 Como vemos, d es una matriz simétrica, con ceros en la diagonal, que R almacena en forma de vector, y para ahorrar espacio solamente almacena el triángulo inferior de la matriz. Si hacemos d[1] obtenemos la distancia (euclídea) entre Austria y Albania. d[1] ## [1] 6.136051 sum((data.protein[&quot;Austria&quot;,] - data.protein[&quot;Albania&quot;,])^2)^(1/2) ## [1] 6.136051 Como vemos, d[1] coincide con la distancia euclídea entre las observaciones (vectores) data.protein[\"Austria\",] y data.protein[\"Albania\",], calculada usando la fórmula. Existen además otras distancias que podemos calcular usando la función dist. d = dist(data.protein, method = &quot;maximum&quot;); d[1] ## [1] 3.410862 max(abs(data.protein[&quot;Austria&quot;,] - data.protein[&quot;Albania&quot;,])) ## [1] 3.410862 dist(data.protein[c(&quot;Albania&quot;, &quot;Austria&quot;),], method = &quot;manhattan&quot;) ## Albania ## Austria 15.97134 sum(abs(data.protein[&quot;Austria&quot;,] - data.protein[&quot;Albania&quot;,])) ## [1] 15.97134 2.1.2 Linkages. ¿Qué especifica el parámetro method en la función hclust? Comparemos los dos dendogramas que se obtienen al variar el parámetro method de la función hclust. par=par(mfrow=c(1,2), cex=0.5) hc1 = hclust(dist(data.protein, method = &quot;euclidean&quot;), method = &quot;complete&quot;) hc2 = hclust(dist(data.protein, method = &quot;euclidean&quot;), method = &quot;single&quot;) plot(hc1, hang=-1) plot(hc2, hang=-1) par(par) Como observamos, hemos obtenido dendogramas muy diferentes al cambiar method=\"complete\" por method = \"single\". En general, una función linkage especifica una similitud ( no necesariamente una distancia en el sentido matemático ) entre dos conjuntos (clusters) de datos. 2.1.3 Los métodos cutree y rect.hclust Independientemente de la distancia (o dis-similitud) que consideremos entre las observaciones, y el método linkage para agrupar, usualmente el objetivo que perseguimos al hacer clúster jerárquico es reportar posibles grupos latentes en las observaciones. Sin embargo, hasta ahora hemos visto cómo obtener un dendogram, pero no cómo decidir qué grupos considerar. La función cutree del paquete básico stats realiza un corte horizontal del dendograma. Podemos proporcionar uno de los dos, un número fijo de grupos mediante el parámetro k, o una altura en la cual cortar con el parámetro h. cutree(hc1, k=3) ## Albania Austria Belgium Bulgaria Czechoslovakia ## 1 2 2 1 2 ## Denmark E Germany Finland France Greece ## 2 2 2 2 1 ## Hungary Ireland Italy Netherlands Norway ## 2 2 1 2 2 ## Poland Portugal Romania Spain Sweden ## 2 3 1 3 2 ## Switzerland UK USSR W Germany Yugoslavia ## 2 2 2 2 1 La función cutree retorna un vector de tipo Named int de longitud es número de observaciones, con los índices de pertenencia a los grupos. Podemos utilizar esta información para trabajar con los diferentes grupos. cut = cutree(hc1, k=5) for (i in 1:5) { write(paste0(&quot;Cluster &quot;,i,&quot;:\\n&quot;, toString(names(which(cut==i))), &quot;\\n-----------------\\n&quot;), &quot;&quot;)} ## Cluster 1: ## Albania, Bulgaria, Greece, Italy, Romania, Yugoslavia ## ----------------- ## ## Cluster 2: ## Austria, Belgium, France, Ireland, Netherlands, Switzerland, UK, W Germany ## ----------------- ## ## Cluster 3: ## Czechoslovakia, E Germany, Hungary, Poland, USSR ## ----------------- ## ## Cluster 4: ## Denmark, Finland, Norway, Sweden ## ----------------- ## ## Cluster 5: ## Portugal, Spain ## ----------------- Otra función que puede ser muy útil para representar la agrupación obtenida es rect.hclust, también del paquete base stats. plot(hc1, hang = -1, cex=0.7) rect.hclust(hc1, k=5) 2.2 Librería protoclust Recientemente, Bien and Tibshirani (2011) han introducido un nuevo tipo de linkage, el minimax linkage. Este tiene la propiedad de que para un corte a altura h, cualquier punto está a distancia menor que h del centro de su cluster. Para utilizar el linkage minimax, debemos instalar el paquete protoclust library(protoclust) La función protoclust retorna un objeto similar a la salida de hclust. pc = protoclust(dist(data.protein, method = &quot;euclidean&quot;)) plot(pc, hang = -1, cex=0.7) rect.hclust(pc, k=3) Podemos visualizar los dendrogramas con factoextra. library(factoextra) res &lt;- hcut(data.protein, hc_func = &quot;hclust&quot;, hc_method = &quot;complete&quot;, k = 5) fviz_dend(res) fviz_dend(res, type = &quot;circular&quot;) References "],
["som.html", "Capítulo 3 Self Organizing Maps 3.1 Librería kohonen", " Capítulo 3 Self Organizing Maps En esta práctica veremos un ejemplo de aplicación de los SOM. Comenzamos leyendo los datos data.protein = read.csv(&quot;data/protein.csv&quot;) rownames(data.protein) = data.protein$Country data.protein$Country = NULL data.protein = scale(data.protein) 3.1 Librería kohonen Ahora crearemos el grid SOM. Generalmente especificamos el tamaño del grid antes de entrenar el modelo. library(kohonen) som_grid = somgrid(xdim = 4, ydim= 4, topo = &quot;hexagonal&quot;) Aquí especificamos la topología de la capa de salida. En este caso, será un grid de 4x4, donde cada neurona tiene 6 vecinos. Si elegimos topo=\"rectangular\" entonces cada neurona tendrá 4 vecinos. A continuación, entrenamos la red. som_model = som(data.protein, grid = som_grid, rlen = 100, keep.data = T) El parámetro rlenespecifica el número de épocas (barridos sobre todo el dataset). Podemos visualizar el proceso de entrenamiento. plot(som_model, type = &quot;changes&quot;) Esto nos permite decidir si hay algún parámetro que debemos variar, como rlen, alpha, radius, etc. som_model = som(data.protein, grid = som_grid, rlen = 600, keep.data = T) plot(som_model, type = &quot;changes&quot;) También podemos ver de forma gráfica el número de veces que cada neurona es activada en el modelo final. plot(som_model, type=&quot;count&quot;) 3.1.1 U-Matrix La U-Matrix (Unified distance matrix o Neighboring distances matrix) es una representación de un SOM, donde las distancias euclídeas entre los pesos de neuronas vecinas son descritas en una imagen en escala de grises. Veamos cómo construir la U-Matrix plot(som_model, type=&quot;dist.neighbours&quot;, palette.name = gray.colors) En un SOM los outliers se manifiestan como nodos cuyos pesos están muy alejados de los pesos de sus nodos vecinos. Por tanto, en la U-matrix los nodos outlies son los más claros. Todas las observaciones que activen nodos claros son observaciones que tienen un patrón muy diferente al resto. Si queremos recuperar las filas que activan nodos más claros, hacemos lo siguiente. Umat = plot(som_model, type=&quot;dist.neighbours&quot;) data = merge(data.frame(obs = rownames(data.protein), nodo = som_model$unit.classif), data.frame(nodo = 1:length(Umat), u = Umat)) summary(data$u) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.213 3.087 5.746 5.896 8.853 11.688 library(dplyr) data %&gt;% arrange(desc(u)) ## nodo obs u ## 1 13 Spain 11.687834 ## 2 13 Portugal 11.687834 ## 3 14 Greece 10.863085 ## 4 15 Albania 9.836145 ## 5 15 Bulgaria 9.836145 ## 6 11 USSR 8.934703 ## 7 10 Italy 8.852920 ## 8 7 France 7.106254 ## 9 7 Switzerland 7.106254 ## 10 12 Hungary 6.780789 ## 11 16 Romania 5.829669 ## 12 16 Yugoslavia 5.829669 ## 13 8 Czechoslovakia 5.746361 ## 14 8 Poland 5.746361 ## 15 3 W Germany 3.874067 ## 16 5 Norway 3.086750 ## 17 5 Denmark 3.086750 ## 18 5 Finland 3.086750 ## 19 5 Sweden 3.086750 ## 20 2 Belgium 2.898122 ## 21 2 Ireland 2.898122 ## 22 2 UK 2.898122 ## 23 4 Austria 2.212600 ## 24 4 E Germany 2.212600 ## 25 4 Netherlands 2.212600 Podemos visualizar también las características (pesos) de cada nodo de salida. plot(som_model, &quot;codes&quot;) 3.1.2 Clusters Una vez que tenemos entrenada la red, cada neurona de la capa de salida tendrá asociados unos pesos, a los que podemos acceder con som_model$codes Estos pesos pueden ser interpretados como puntos en el espacio de los datos originales, por lo que podemos aplicarle a estos las técnicas de segmentación que ya conocemos (e.g. kmeans o hclust). En primer lugar, calculamos las distancias entre los nodos del mapa (en el espacio de los datos originales) dist = object.distances(som_model, &quot;codes&quot;) A continuación, agrupamos estos nodos usando cluster jerárquico. hc = hclust(dist, method = &quot;ward.D2&quot;) Visualizamos para encontrar el número de clusters. library(factoextra) fviz_dend(hc, show_labels = F) som.hc = cutree(hc, k = 4) plot(som_model, type=&quot;dist.neighbours&quot;, palette.name = gray.colors) add.cluster.boundaries(som_model, som.hc) colors = topo.colors(3) plot(som_model, type=&quot;mapping&quot;, bgcol = colors[som.hc], pch = &quot;&quot;, main = &quot;Clusters&quot;) add.cluster.boundaries(som_model, som.hc) "],
["clústers-basados-en-densidades.html", "Capítulo 4 Clústers basados en densidades 4.1 Librería dbscan", " Capítulo 4 Clústers basados en densidades 4.1 Librería dbscan DBSCAN (Density-Based Spatial Clustering and Application with Noise) es un algoritmo de cluster basado en densidades, que puede ser utilizado para identificar clusters que contengan patrones, ruido y outliers. Mediante la grupación basada en densidades, se localizan áreas de mayor densidad con respecto al resto de los datos. Los puntos que quedan fuera de estas áreas de densidad son considerados ruido o puntos de frontera. Si quisiéramos enseñar a una máquina a “observar” patrones como los seres humanos lo hacemos, probablemente deberíamos comenzar plantéandonos un método como DBSCAN. El algoritmo DBSCAN se basa en la noción intuitiva detrás de lo que es cluster y lo que es ruido. Para los puntos que pertenecen a un cluster, hay una “vecindad” de radio dado, tal que esta vecindad contiene un número mínimo de puntos que también pertenecen al mismo cluster. Por ejemplo, la siguiente imagen puede ser fácilmente segnemtada por un ser humano con una simple pasada. ¿Pudiésemos utilizar análisis cluster para determinar quiénes son los personajes de esta historia? library(dplyr) library(ggplot2) points = read.table(file = &quot;data/arte-rupestre.txt&quot;, header = T) ggplot(points, aes(x=x, y=y)) + geom_point(size = 0.5) + theme_void() Veamos qué sucede si intentamos particionar estos puntos usando k-means. library(factoextra) set.seed(0) km.res = kmeans(points, 3, nstart = 9) fviz_cluster(km.res, points, geom = &quot;point&quot;, ellipse = F, show.clust.cent = F, palette=&quot;jco&quot;, ggtheme = theme_classic() ) Como podemos observar, k-means no identifica apropiadamente estos clusters con formas arbitrarias, incluso cuando le decimos exactamente cuántos grupos verdaderos hay. 4.1.1 ¿Cómo funciona? El objetivo de DBSCAN es identificar regiones densas, que pueden ser medidas según el número de objetos cercanos a un punto dado. DBSCAN requiere dos hiperparámetros importantes, epsilon (eps) Define el radio de la vecindad alrededor de un punto \\(x\\), comúnmente llamada \\(\\epsilon-\\)vecindad. minimum points (minPts) Número mínimo de vecinos en un radio eps. Esto da lugar a una clasificación en tres tipos de puntos. Dado \\(x\\) que pertenece al conjunto de datos, \\(x\\) es un punto interior si tiene más de minPts puntos en su \\(\\epsilon-\\)vecindad. \\(x\\) es un punto frontera si no es interior, pero pertenece a la \\(\\epsilon-\\)vecindad de algún punto interior. \\(x\\) es un outlier si no es punto interior ni frontera. La siguiente figura muestra los diferentes tipos de puntos que podemos tener. En este ejemplo, \\(x\\) es un punto interior (porque tiene 6 puntos a distancia menor que eps), \\(y\\) es un punto frontera (porque pertenece a la \\(\\epsilon-\\)vecindad de \\(x\\)) y \\(z\\) es un outlier. El algoritmo DBSCAN se basa en las siguientes nociones de conectividad entre puntos. Direct density reachable (\\(\\rightarrow\\)) Decimos que \\(B \\rightarrow A\\) si \\(A\\) pertenece a la \\(\\epsilon-\\)vecindad de \\(B\\) \\(B\\) es un punto interior Density reachable (\\(\\rightarrow \\cdot \\rightarrow\\)) Decimos que \\(B \\rightarrow \\cdot \\rightarrow A\\) si existe un conjunto de puntos interiores que lleva de \\(B\\) a \\(A\\). Density connected \\(A\\) y \\(B\\) son density connected si existe un punto interior \\(C\\) tal que \\(B \\leftarrow \\cdot \\leftarrow C \\rightarrow \\cdot \\rightarrow A\\). Un cluster basado en densidad se define como un conjunto de puntos density connected. La idea del algoritmo DBSCAN es la siguiente. Para un punto inicial \\(x_0\\) encontrar todos los puntos en su \\(\\epsilon-\\)vecindad. Cada punto con un número de vecinos mayor o igual que minPts es marcado como interior, de lo contrario es marcado como visitado. Para cada punto interior, si este no está en ningún cluster, crear un nuevo cluster. Recursivamente, encontrar todos los puntos density connected con él, y asignarlos a su mismo clúster. Iterar 1-2 en los puntos restantes no visitados. Los puntos que no pertenecen a ningún cluster son marcados como outliers. Visualización paso a paso: https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ 4.1.2 Ventajas A diferencia de k-means, DBSCAN no requiere un número prefijado de clusters. DBSCAN puede lidiar con cualquier forma en los clústers, no necesariamente circular. DBSCAN identifica los outliers. 4.1.3 Utilización La función dbscan, del paquete del mismo nombre, es una implementación optimizada del algoritmo DBSCAN. Para comprobar si el paquete no está instalado e instalarlo, ejecutamos la línea siguiente. if(!require(&quot;dbscan&quot;))install.packages(&quot;dbscan&quot;) Vamos a aplicar el algoritmo DBSCAN al conjunto de puntos de nuestro ejemplo. library(dbscan) db = dbscan(points, eps = 15, minPts = 6) fviz_cluster(db, data=points, stand = FALSE, ellipse = FALSE, show.clust.cent = FALSE, geom = &quot;point&quot;,palette = &quot;jco&quot;, ggtheme = theme_classic()) Ups! DBSCAN solamente identifica un cluster en este ejemplo. Veamos si podemos encontrar los hiperparámetros eps y minPts óptimos. 4.1.4 Determinando el eps óptimo. El método para determinar el mejor epsconsiste en, fijado el valor de minPts, calcular la distancia media de cada punto a sus \\(k=\\)minPts vecinos más cercanos. Luego este promedio para cada punto es mostrado en orden ascendente, y el valor de eps se escoje mirando el primer codo. En nuestro ejemplo, dist &lt;- kNNdist(points, k=10) dist &lt;- sort(dist) ggplot() + aes(x = 1:length(dist), y = dist) + geom_line()+ geom_hline(yintercept = 6, linetype = &quot;dashed&quot;) library(dbscan) db = dbscan(points, eps = 6, minPts = 10) fviz_cluster(db, data=points, stand = FALSE, ellipse = FALSE, show.clust.cent = FALSE, geom = &quot;point&quot;,palette = &quot;jco&quot;, ggtheme = theme_classic()) "],
["aprendizaje-semi-supervisado.html", "Capítulo 5 Aprendizaje semi-supervisado 5.1 Librería s2net", " Capítulo 5 Aprendizaje semi-supervisado En esta sección estudiaremos un tipo de problema intermedio entre supervisado y no supervisado, que es muy común en la práctica. 5.1 Librería s2net La librería s2net de R resuelve un problema de optimización similar al elastic-net, pero para tratar con datos semi-supervisados. No obstante, es más general, e incluye el caso de datos supervisados también. Fue concebido específicamente para resolver problemas de transfer-learning en datos de alta dimensión, donde además de ajustar un modelo lineal, se desea incluir solamente un subconjunto de variables explicativas en el modelo (como hace elastic-net). Más información aquí: https://github.com/jlaria/s2net You can install the released version of s2net from CRAN with: install.packages(&quot;s2net&quot;) The development version can be installed with: devtools::install_github(&quot;jlaria/s2net&quot;, build_vignettes = TRUE) This is a basic example which shows you how to use the package. Detailed examples can be found in the documentation and vignettes. library(s2net) # Auto-MPG dataset is included for benchmark data(&quot;auto_mpg&quot;) Semi-supervised data is made of a labeled dataset xL, the labels yL, and unlabeled data xU. Package s2net includes the function s2Data to process semi-supervised datasets. head(auto_mpg$P2$xL, 2) # labeled data ## displacement horsepower weight acceleration year origin ## 15 113 91 2372 15.0 70 3 ## 19 97 84 2130 14.5 70 3 head(auto_mpg$P2$yL, 2) # labels ## [1] 24 27 head(auto_mpg$P2$xU, 2) # unlabeled data ## displacement horsepower weight acceleration year origin ## 1 307 17 3504 12.0 70 1 ## 2 350 35 3693 11.5 70 1 train = s2Data(auto_mpg$P2$xL, auto_mpg$P2$yL, auto_mpg$P2$xU, preprocess = TRUE) head(train$xL, 2) ## displacement horsepower weight acceleration year origin2 ## 15 0.1788500 1.0544632 0.1799762 -0.6392182 -1.878311 -0.6575667 ## 19 -0.5510247 0.7397884 -0.5209896 -0.8471591 -1.878311 -0.6575667 ## origin3 ## 15 1.356622 ## 19 1.356622 The data is centered and scaled, and factor variables are automatically converted to numerical dummies. Constant columns are also removed. If we wanted to use validation/test data, we must pre-process it according to the training data, with: valid = s2Data(auto_mpg$P2$xU, auto_mpg$P2$yU, preprocess = train) There are two ways to fit a semi-supervised elastic-net using s2net. The easiest way is using the function s2netR, that returns an object of S3 class s2netR. model = s2netR(train, params = s2Params(lambda1 = 0.01, lambda2 = 0.01, gamma1 = 0.01, gamma2 = 100, gamma3 = 0.1)) class(model) ## [1] &quot;s2netR&quot; model$beta ## [,1] ## [1,] -0.28152012 ## [2,] 0.04116177 ## [3,] -3.02848437 ## [4,] 0.61602553 ## [5,] 3.65674054 ## [6,] 0.71547766 ## [7,] 0.43169913 ypred = predict(model, valid$xL) "],
["hyperparam.html", "Capítulo 6 Selección de hiperparámetros 6.1 Librerías tune y dials", " Capítulo 6 Selección de hiperparámetros En esta sección estudiaremos un enfoque moderno a la selección de hyperparámetros en algoritmos supervisados. Nos enfocaremos en el elastic-net, pero los resultados se pueden extender a cualquier algoritmo supervisado soportado por la librería parsnip. Comenzamos cargando y preparando los datos. Usaremos unos datos simulados. load(&quot;data/sim_data.RData&quot;) Primero vamos a separar los datos en training y test. Esta vez, lo haremos con la librería rsample. library(rsample) set.seed(0) split &lt;- initial_split(datos, prop = 0.5) train_data &lt;- training(split) test_data &lt;- testing(split) Para escalar los datos, utilizaremos la librería recipes. library(recipes) data_rec &lt;- recipe(y~., data = train_data) %&gt;% step_normalize(all_numeric(), -all_outcomes()) %&gt;% prep(train_data) train_data &lt;- bake(data_rec, train_data) test_data &lt;- bake(data_rec, test_data) A continuación creamos el modelo usando la librería parsnip. Podemos especificar los hyper-parámetros ahora o más adelante. library(parsnip) model &lt;- linear_reg(penalty = 0.01, mixture = 0.5) %&gt;% #elasticnet set_engine(&quot;glmnet&quot;) Para ajustar el modelo, podemos hacer model &lt;- fit(model, y~., train_data) Para obtener predicciones, hacemos ypred &lt;- predict(model, test_data) En este caso, podemos calcular el error test usando la librería yardstick. library(yardstick) rmse(data.frame(truth = test_data$y, estimate = ypred$.pred), truth, estimate) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.30 6.1 Librerías tune y dials Los datos test en la práctica son desconocidos. ¿Cómo sabemos que valores de penalty y mixture nos darán mejores resultados? Las librerías tune y dials nos brindan funcionalidades para seleccionar de forma óptima los hyperparámetros de un modelo. 6.1.1 Grid search Para utilizar grid search, primero creamos el modelo parsnip, pero esta vez especificamos que no conocemos los hyper-parámetros óptimos, y los dejamos a la librería tune para escoger. library(tune) model &lt;- linear_reg(penalty = tune(), mixture = tune()) %&gt;% set_engine(&quot;glmnet&quot;) A continuación, creamos las particiones bootstrap para entrenar los diferentes modelos. train_rs &lt;- bootstraps(train_data, times = 2) Especificamos qué metrica queremos optimizar, en este caso como es un problema de regresión, minimizaremos el rmse. Podemos incluir varias y luego optimizar con respecto a alguna. metrics &lt;- metric_set(rmse) Finalmente, creamos el grid de hyper-parámetros los cuales vamos a probar en las diferentes particiones bootstrap. library(dials) params &lt;- parameters( penalty(), mixture() ) grid &lt;- grid_regular(params, levels = 10) Para entrenar los modelos, usamos la función tune_grid de la librería tune. ctrl &lt;- control_grid(verbose = FALSE) result &lt;- tune_grid( y~., model = model, resamples = train_rs, metrics = metrics, control = ctrl, grid = grid ) result %&gt;% show_best(maximize = FALSE) ## # A tibble: 5 x 7 ## penalty mixture .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 0.222 rmse standard 1.05 2 0.0199 ## 2 0.0774 0.111 rmse standard 1.08 2 0.00585 ## 3 0.0000000001 0.111 rmse standard 1.08 2 0.00597 ## 4 0.00000000129 0.111 rmse standard 1.08 2 0.00597 ## 5 0.0000000167 0.111 rmse standard 1.08 2 0.00597 Podemos evaluar cómo lo hace esta combinación de hyper-parámetros en los datos test. best_config &lt;- show_best(result, maximize = FALSE) model &lt;- linear_reg(penalty = best_config$penalty[1], mixture = best_config$mixture[1]) %&gt;% set_engine(&quot;glmnet&quot;) %&gt;% fit(y~., train_data) ypred &lt;- predict(model, test_data) rmse(data.frame(truth = test_data$y, estimate = ypred$.pred), truth, estimate) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.19 6.1.2 Random Search Otro enfoque (superior a grid search) es dejar a tune y dials escoger los valores de los hiperparámetros para cada iteración. Ambos grid y random search se pueden ejecutar en paralelo. Para hacer random search procedemos como grid search pero en lugar de especificar un grid, especificamos un número de combinaciones. model &lt;- linear_reg(penalty = tune(), mixture = tune()) %&gt;% set_engine(&quot;glmnet&quot;) result &lt;- tune_grid( y~., model = model, resamples = train_rs, metrics = metrics, control = ctrl, grid = 100 ) result %&gt;% show_best(maximize = FALSE) ## # A tibble: 5 x 7 ## penalty mixture .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.162 0.715 rmse standard 1.03 2 0.0148 ## 2 0.488 0.545 rmse standard 1.03 2 0.0369 ## 3 0.354 0.883 rmse standard 1.05 2 0.0622 ## 4 0.148 0.619 rmse standard 1.05 2 0.0291 ## 5 0.527 0.655 rmse standard 1.07 2 0.0568 best_config &lt;- show_best(result, maximize = FALSE) model &lt;- linear_reg(penalty = best_config$penalty[1], mixture = best_config$mixture[1]) %&gt;% set_engine(&quot;glmnet&quot;) %&gt;% fit(y~., train_data) ypred &lt;- predict(model, test_data) rmse(data.frame(truth = test_data$y, estimate = ypred$.pred), truth, estimate) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.18 6.1.3 Optimización Bayesiana model &lt;- linear_reg(penalty = tune(), mixture = tune()) %&gt;% set_engine(&quot;glmnet&quot;) train_rs &lt;- bootstraps(train_data, times = 10) metrics &lt;- metric_set(rmse) ctrl &lt;- control_bayes(verbose = FALSE) result &lt;- tune_bayes( y~., model = model, resamples = train_rs, metrics = metrics, iter = 100, control = ctrl ) result %&gt;% show_best(maximize = FALSE) ## # A tibble: 5 x 8 ## penalty mixture .iter .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 3.92e- 1 0.168 0 rmse standard 1.23 10 0.0339 ## 2 1.49e- 4 0.458 0 rmse standard 1.26 10 0.0415 ## 3 1.06e-10 0.595 5 rmse standard 1.27 10 0.0442 ## 4 3.10e- 2 0.258 8 rmse standard 1.27 10 0.0412 ## 5 1.34e- 7 0.685 0 rmse standard 1.27 10 0.0458 best_config &lt;- show_best(result, maximize = FALSE) model &lt;- linear_reg(penalty = best_config$penalty[1], mixture = best_config$mixture[1]) %&gt;% set_engine(&quot;glmnet&quot;) %&gt;% fit(y~., train_data) ypred &lt;- predict(model, test_data) rmse(data.frame(truth = test_data$y, estimate = ypred$.pred), truth, estimate) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.23 "],
["references.html", "References", " References "]
]
