# Clústers partitivos{#kmeans}

Utilizaremos los datos de @weber1974structure sobre consumo de proteínas en los países europeos en la década de los '70 del pasado siglo.

Comenzamos por cargar los datos en **R**.

```{r}
data.protein = read.csv("data/protein.csv")
```

En este caso, el archivo que queremos importar en **R** se encuentra en un recurso web, en lugar de la ruta local usual. Una vez importados los datos, podemos ver de qué se tratan.

```{r, message=F}
library(dplyr)
library(magrittr)

data.protein %>% head
```


```{r}
dim(data.protein)
```

Tenemos 25 países y 10 variables, aunque una de estas variables es `Country`, que contiene el identificador de cada fila. Debemos transformar este dataset para deshacernos de la columna `Country` sin perder la información de las filas. Esto lo hacemos con el comando `rownames` (que es de lectura y asignación).

```{r}
rownames(data.protein)
```

```{r}
rownames(data.protein) = data.protein$Country
```

Si ahora vemos nuestro dataset, observamos que la información de la variable `Country` está repetida en los nombres de las filas.

```{r}
data.protein %>% head
```

Por tanto, podemos eliminar esa columna

```{r}
data.protein$Country = NULL
```

Como podemos observar, aunque posiblemente las variables estén medidas en las mismas unidades, no se encuentran en el mismo rango de valores. Por ejemplo, `Cereals` toma valores altos comparada con el resto. Para evitar que la escala individual de las variables afecten el resultado final, escalamos previamente la matriz de datos.


```{r}
data.protein %<>% scale %>% as.data.frame
data.protein %>% head
```


## K medias

*K-means* es, probablemente, uno de los algoritmos de clúster más eficientes computacionalmente. Sin embargo, tiene dos desventajas fundamentales.

1. Se considera únicamente la distancia euclídea.
2. Hay que especificar desde el comienzo el número de clusters que queremos.

El funcionamiento de este algoritmo se ilustra [aquí](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/).

Para hacer k-medias en R utilizamos la función `kmeans` del paquete base `stats`. En este caso, especificaremos 5 clusters.

<!-- ```{r} -->
<!-- data.protein = read.csv("data/protein.csv") -->
<!-- rownames(data.protein) = data.protein$Country -->
<!-- data.protein$Country = NULL -->
<!-- data.protein = as.data.frame(scale(data.protein)) -->
<!-- ``` -->


```{r}
km = kmeans(data.protein, centers = 5)
km$cluster 
```

Como vemos, la función `kmeans` devuelve una agrupación, de hecho, `km$cluster` tiene una estructura que comparten todos los algoritmos de agrupación (observación, clúster).

## Librería `factoextra`

`factoextra` es un paquete para extraer y visualizar las salidas de distintos análisis exploratorios en R.
<!-- Para instalarlo desde [github](https://github.com/kassambara/factoextra), primero necesitamos instalar la librería `devtools`. -->
Para visualizar los grupos resultantes, podemos utilizar la función `fviz_cluster` de la librería `factoextra`. Esta función visualizará los datos utilizando las dos primeras componentes principales.

```{r}
library(factoextra)
fviz_cluster(km, data=data.protein, labelsize = 8)

 # Interactivo
library(plotly)
fviz_cluster(km, data=data.protein, labelsize = 8)%>%
ggplotly()
```


## Variación de información

¿Cómo podríamos medir cuán diferentes son dos agrupaciones dadas sobre los mismos datos? Por ejemplo, la salida de `km$cluster` escalando y sin escalar los datos.

La *variación de información* proporciona una medida para decir cuánta información comparten (o en este caso, no comparten) dos particiones de los datos. De hecho, es una distancia en el sentido matemático estricto de distancia. 

```{r}
cluster_scale <- km$cluster

# Repetimos el mismo análisis pero sin escalar
data.protein = read.csv("data/protein.csv")
rownames(data.protein) = data.protein$Country
data.protein$Country = NULL
km = kmeans(data.protein, centers = 5)

cluster_no_scale <- km$cluster

```

El paquete `mcclust` incluye una función para calcular la variación de información.

```{r}
library(mcclust)
vi.dist(cluster_scale, cluster_no_scale, parts = TRUE)
```

¿ Con una agrupación aleatoria qué obtendríamos ?

```{r}
cluster_rand <- sample(5, length(cluster_scale), replace = TRUE)

vi.dist(cluster_rand, cluster_scale)
```


## Minibatch kmeans

*MiniBatch k-means* ha sido propuesto como una alternativa al algoritmo *k-means* para agrupar datos masivos. La ventaja de *MiniBatch k-means* es que reduce el coste computacional al no utilizar todos los datos en cada iteración, sino una muestra aleatoria de tamaño fijo.

En esta sección ilustraremos el uso de *MiniBatch k-means* en segmentación de imágenes (*Color Quantization*). Utilizaremos las librerías de R `ClusterR` para el algoritmo, y `OpenImageR` para representar las imágenes.

```{r}
library(ClusterR)
library(OpenImageR)
```

En este ejemplo utilizaremos una imagen de arte rupestre. Primero descargamos y leemos la imagen.

```{r}
img = readImage("data/beach.png")
dim(img)
```

```{r, include=FALSE}
# img1 <- OpenImageR::resizeImage(img, 299, 299)
# imageShow(img1)
# OpenImageR::writeImage(img1, "data/beach.png")
```


Podemos observar que la imagen tiene una resolución de `r dim(img)` píxeles, separados en tres canales de colores (RGB), por lo que en realidad tenemos `r prod(dim(img)[1:2])` datos y `3` variables. Para mostrar la imagen en pantalla utilizamos la función `imageShow`.

```{r}
imageShow(img)
# img <- RGB_to_HSV(img)
```

A continuación, convertimos nuestra matriz `381x514x3` en otra matriz de dimensión `195834x3`.

```{r}
img.vector = apply(img, 3, as.vector)
dim(img.vector)
```

Ya estamos en condiciones de hacer MiniBatch K-means a nuestra imagen.

```{r}
mbkm = MiniBatchKmeans(img.vector, clusters = 3)
```

A continuación, vamos a sustituir la información de cada píxel por el centro del clúster al que pertenece. De esta forma obtendremos una imagen que tiene solamente 10 colores.

```{r}
mb.pred = predict_MBatchKMeans(img.vector, mbkm$centroids)
new.img = mbkm$centroids[mb.pred, ] 
```

Devolvemos la imagen a su estructura original para poderla representar.

```{r}
dim(new.img) = c(nrow(img), ncol(img), 3) 
imageShow(new.img)
```

## K mediano

*Partition around medoids* puede verse como una versión robusta de *kmeans*, ya que los centroides son observaciones en lugar de promedios de observaciones. Esto da lugar a clusters más interpretables. Adicionalmente, su implementación tiene ventajas con respecto a *kmeans*, por ejemplo, no se limita a la distancia euclídea.

Para este algoritmo, utilizaremos la función `pam` de la librería `cluster`.

```{r}
library(cluster)
pm = pam(data.protein, k=5)
knitr::kable(pm$medoids)
```

El valor que devuelve la función `pam` es similar al que devuelve `kmeans`, excepto que en lugar de tener la propiedad `centers`, ahora tenemos `medoids`.

Podemos visualizar la agrupación resultante.

```{r}
fviz_cluster(pm, data=data.protein, labelsize = 8)
```


### `CusterR::clara`

Como es lógico, *pam* es computacionalmente más costoso que *k-means*, pues calcular el *medoid* es mucho más difícil que hacer un promedio. Existe una alternativa eficiente a *k-medoids*, *clara* para agrupar grandes volúmenes de datos. Se basa en agrupar primero una muestra de los datos originales y luego asignar los datos restantes a estos grupos.

Utilizaremos la función `Clara_Medoids` del paquete `ClusterR`, porque vamos a trabajar con la misma foto que usamos para ilustrar `MiniBatchKmeans`.

```{r}
clara.m = Clara_Medoids(img.vector, clusters = 10, samples = 10, sample_size = 0.001)
```

Hay que ser muy cuidadosos aquí con los parámetros, `sample` y `sample_size`, porque tenemos muchos datos, y esta función es muy costosa computacionalmente si aumentamos estos parámetros.

A continuación asignamos cada pixel a su color correspondiente, y mostramos la nueva imagen.

```{r}
clara.pred = predict_Medoids(img.vector, clara.m$medoids)
new.img = clara.m$medoids[clara.pred$clusters, ] 
dim(new.img) = c(nrow(img), ncol(img), 3) 
imageShow(new.img)
```

## Selección de K

En esta sección estudiaremos tres técnicas para calcular el número óptimo de clusters. Primero, vamos a sacar una muestra de nuestros datos de menos tamaño, ya que los cálculos que haremos serán más costosos.

```{r}
set.seed(1)
img.vector.small = img.vector[sample(nrow(img.vector),1000),]
```

### Método del codo

La idea básica de los algoritmos partitivos es obtener cluster con la mínima WSS (within-cluster sum of squares), que mide cuán compactos son los clusters. Pudiéramos intentar quedarnos con el número de clusters `nclust` que minimiza este valor. Sin embargo, WSS siempre decrece a medida que consideramos un mayor número de grupos. 

El método del codo mira el valor de WSS con respecto al número de grupos considerados, y busca el primer punto en que hay un cambio brusco en la curva, es decir, que adicionar un grupo nuevo no mejora demasiado con respecto a lo que ya había.

```{r}
library(factoextra)

fviz_nbclust(img.vector.small, kmeans, method = "wss") +
    geom_vline(xintercept = 3, linetype = 2)+
  labs(subtitle = "Elbow method")

```

### Método average silhouette

La idea básica de este método es medir la calidad de la agrupación en función de cuán bien encierra los datos en los diferentes grupos. ¿Debería cambiar mucho la silueta del cluster si quitamos alguna de sus observaciones?

El método de la silueta promedio calcula la silueta de los grupos para distintos números de grupos `nclust`. El mejor número es aquel que maximiza la silueta.

```{r}
fviz_nbclust(img.vector.small, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

### Método Gap statistic

Este se puede considerar el más formal de los métodos, y puede ser aplicado a cualquier método de clustering, incluyendo clúster jerárquico.

```{r}
set.seed(3)
gap_stat = cluster::clusGap(img.vector.small, FUN = kmeans, K.max = 20)
fviz_gap_stat(gap_stat, maxSE = list(method="Tibs2001SEmax"))+labs(subtitle = "Gap statistic method")

```

<!-- ## spark kmeans -->
